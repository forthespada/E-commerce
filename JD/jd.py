# -*- coding: utf-8 -*-import requests,re,json,pymongo,timefrom lxml import etreefrom requests.exceptions import RequestExceptionfrom pyquery import PyQuery as pqimport numpy as npimport pandas as pdMONGO_URL = 'localhost'MONGO_DB = 'jd'MONGO_TABLE = 'jd_comment'MONGO_TABLE_URL = 'jd_url'def get_headers():    # 各种PC端    user_agent_list_2 = [        # Opera        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60",        "Opera/8.0 (Windows NT 5.1; U; en)",        "Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50",        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50",        # Firefox        "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0",        "Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10",        # Safari        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2",        # chrome        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36",        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",        "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16",        # 360        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36",        "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko",        # 淘宝浏览器        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11",        # 猎豹浏览器        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER",        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)",        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)",        # QQ浏览器        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)",        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)",        # sogou浏览器        "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0",        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)",        # maxthon浏览器        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36",        # UC浏览器        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36",    ]    #各种移动端    user_agent_list_3 = [        # IPhone        "Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",        # IPod        "Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",        # IPAD        "Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5",        "Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",        # Android        "Mozilla/5.0 (Linux; U; Android 2.2.1; zh-cn; HTC_Wildfire_A3333 Build/FRG83D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",        "Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",        # QQ浏览器 Android版本        "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",        # Android Opera Mobile        "Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10",        # Android Pad Moto Xoom        "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13",        # BlackBerry        "Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+",        # WebOS HP Touchpad        "Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0",        # Nokia N97        "Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124",        # Windows Phone Mango        "Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)",        # UC浏览器        "UCWEB7.0.2.37/28/999",        "NOKIA5700/ UCWEB7.0.2.37/28/999",        # UCOpenwave        "Openwave/ UCWEB7.0.2.37/28/999",        # UC Opera        "Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999"    ]    # 一部分 PC端的    user_agent_list_1 = [        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",        "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",        "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",        "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"    ]    user_agent_list = user_agent_list_1+user_agent_list_2+user_agent_list_3;    UserAgent = np.random.choice(user_agent_list)    headers = {'User-Agent': UserAgent}    return headersdef distinct_url(urls,total_list_urls): #链接去重	for j in range(len(urls)):		i = urls[j]		i = i.replace("[",'')		i = i.replace("]","")		if i in total_list_urls:			continue		else:			total_list_urls.append(i)	# print(total_list_urls)	# return total_urlsdef get_one_product_url_from_mongo():	client = pymongo.MongoClient(MONGO_URL)	db = client[MONGO_DB]	try:		product_url = db[MONGO_TABLE_URL].find_one({},{"product_url":{"$slice":1}})		product_url = product_url['product_url']		db[MONGO_TABLE_URL].delete_one({"product_url":product_url}) #取出元素后删除		return product_url	except:		print("表jd_url取出数据错误")def get_url_num_from_mongo():	client = pymongo.MongoClient(MONGO_URL)  	# 连接数据库	db = client[MONGO_DB]	table = db[MONGO_TABLE_URL] 	# 读取数据	data = pd.DataFrame(list(table.find())) 	# 选择需要显示的字段	try:		data = data[['product_url']] #data[['_id'，'product_url']]		data = data['product_url']		print("表jd_url中存在商品链接共",len(data),"个，开始爬取商品")		return len(data)	except:		print("表jd_url中不存在元素")		return Nonedef write_product_url_to_mongo(product_urls):	client = pymongo.MongoClient(MONGO_URL)	db = client[MONGO_DB]	result =[]	for product_url in product_urls:		product_url = product_url.replace('\'','')		product_url = 'https://item.jd.com/'+ product_url +'.html'		product_url = {'product_url':product_url}		result.append(product_url)	if db[MONGO_TABLE_URL].insert_many(result):		pass	else:		print("插入链接失败",result)		# product_url = {'product_url':product_url}		# if db[MONGO_TABLE_URL].update({'product_url':product_url['product_url']},{'$set':product_url},True):		# 	pass		# else:		# 	print("插入商品链接失败:",product_url)def write_product_to_mongo(url,result): # 插入商品信息成功	product_id = url.split('/')[-1].split('.')[0]	product_id = {'product_id':product_id}	result = {'product':result}	client = pymongo.MongoClient(MONGO_URL)	db = client[MONGO_DB]	if db[MONGO_TABLE].update({'product_id':product_id['product_id']},{'$set':product_id},True) and db[MONGO_TABLE].update({'product_id':product_id['product_id']},{'$set':result},True):			pass	else:		print("插入商品信息失败，商品url:",url)def write_comment_to_mongo(url,result): # 插入评论信息	product_id = url.split('=')[1].split('&')[0]	product_id = {'product_id':product_id}	for i in range(len(result)):		comment = {'comment':result[i]}		client = pymongo.MongoClient(MONGO_URL)		db = client[MONGO_DB]		if db[MONGO_TABLE].update({'product_id':product_id['product_id']},{'$set':product_id},True) and db[MONGO_TABLE].update({'product_id':product_id['product_id']},{'$addToSet':comment},True):			pass		else:			print("插入商品评论失败，评论url:",url)def write_product_comments_to_mongo(result): # 插入评论信息	# product_id = url.split('/')[-1].split('.')[0]	# result['product_id'] = product_id	# print(result)	client = pymongo.MongoClient(MONGO_URL)	db = client[MONGO_DB]	if db[MONGO_TABLE].insert_one(result):		pass	else:		print("插入商品评论失败，商品url:","https://item.jd.com/"+result['productid']+".html")def get_list_url(): #京东全站分类的url	sort_url = 'https://www.jd.com/allSort.aspx'	content = parse_page(sort_url)	reg = re.compile(r'<a href="//list\.jd\.com/list\.html\?cat=(.*?)"',re.S)	list_urls = reg.findall(content)	for i in range(len(list_urls)):		list_urls[i] = 'https://list.jd.com/list.html?cat=' + list_urls[i]	print("京东商品类别一共有",len(list_urls),"个！")	return list_urlsdef parse_comment_page(url,proxies): # 评论界面可能获取失败或者当前界面本来就为空即评论没有了	headers = get_headers()	for i in range(3):		try:			content = requests.get(url,headers=headers,timeout=5,proxies=proxies).text			if content:				comment_result = json.loads(content)				if len(comment_result['comments']) != 0:					print(url)					# time.sleep(np.random.rand()/100)					return content		except:			pass	return Nonedef parse_page(url):	for i in range(5):		headers = get_headers()		try:			response = requests.get(url,headers=headers,timeout=5)			if '京东首页' in response.text:				time.sleep(np.random.rand()/10)				return response.text		except:			pass	return Nonedef get_ip():	order = "9369fe38a13b2d743a03755dfc54bdf1";	# 获取IP的API接口	apiUrl = "http://dynamic.goubanjia.com/dynamic/get/" + order + ".html?sep=3&ttl=1";	res = requests.get(apiUrl).content.decode()	# 按照\n分割获取到的IP	ips = res.split('\n');	return ipsdef ip_test(ip):	proxies = {	"http": "http://" + ip,				"https": "https://" + ip,				}	try:		response = requests.get('https://www.baidu.com/',proxies = proxies,timeout = 5)		if response.status_code == 200:			return True		else:			return False	except:		return Falsedef get_total_page_num(list_url): # 查询总共需要爬去的总页数	headers = get_headers()	try:		response = requests.get(list_url,headers=headers)		if response.status_code == 200:			page_num_reg = re.compile(r'class="p-skip".*?共<b>(.*?)</b>页',re.S)			total_page_num = page_num_reg.findall(response.text)[0]			print("此类商品共有",total_page_num,"页！")			return total_page_num	except TimeoutError: # 网速过慢容易引发错误，加上try,错了就再次发起请求		get_total_page_num() # 再次请求def get_total_product_id(product_url,page_num): # 获取京东某页全部商品id	page_url = product_url +"&page="+ str(page_num)	content = parse_page(page_url)	selector = etree.HTML(content)	productidlist = []	items = selector.xpath('//*[@id="plist"]/ul/li')	for item in items:		productid = item.xpath('div/@data-sku')		if len(productid) != 1:			productid= item.xpath('div/div/div[@class="gl-i-tab-content"]/div[2]/@data-sku')		productidlist.append(str(productid))	return productidlistdef extract(url, content):	headers = get_headers()	try:		product_id = url.split('/')[-1].split('.')[0]		doc = pq(content)		#商品类别		product_categories = []		categoryitems = doc.find('#crumb-wrap > div > div.crumb.fl.clearfix > div.item').items()		for item in categoryitems:			categoryitem = item.find('a').text()			if len(categoryitem) != 0:				product_categories.append(categoryitem)		category3 = doc.find('#crumb-wrap > div > div.crumb.fl.clearfix > div.item.ellipsis').text()		product_categories.append(category3)		# print(product_categories)		#商品名，描述		product_name = doc.find('#name > div.sku-name').text()		if len(product_name)== 0:			 product_name = doc.find('body > div:nth-child(9) > div > div.itemInfo-wrap > div.sku-name').text()		product_description = doc.find('#detail > div.tab-con > div:nth-child(1) > div.p-parameter').text()		product_description = product_description.replace('\n','  ').replace('\xa0','').replace('>>','')		# print(product_name)		# print(product_description)		# product_price = doc.find('body > div:nth-child(11) > div > div.itemInfo-wrap > div.summary.summary-first > div > div.summary-price.J-summary-price > div.dd > span.p-price').text()		#商品价格，这里需要再次进行价格接口的获取，JD价格接口是固定的，很好发现规律的		product_price_url = "https://c0.3.cn/stock?skuId=" + product_id +"&cat=1320,1583,1592&venderId=13572&area=1_72_2799_0&buyNum=1&choseSuitSkuIds=&extraParam={%22originid%22:%221%22}&ch=1&fqsp=0&pduid=1540516052714659336599&"		product_price_content = requests.get(product_price_url,headers=headers).text		product_price_result = json.loads(product_price_content)		product_price = product_price_result['stock']['jdPrice']['p']		# print(product_price)		product = {			"product_categories":product_categories,			"product_name":product_name,			"product_description":product_description,			"product_price":product_price		}		#print(product)		return product	except:		print("商品信息获取失败！",url)		return Nonedef reviewExtract(url, content):	try:		commentlist = []		comment_result = json.loads(content)		for j in range(0,10): #分别解析每一页10个评论的数据，获得评论文本，时间，分数，点赞数，评论id			try:				comment_content = comment_result['comments'][j]['content']				comment_time = comment_result['comments'][j]['creationTime']				comment_user_id = comment_result['comments'][j]['id']				comment_score = comment_result['comments'][j]['score']				comment_like_num=comment_result['comments'][j]['usefulVoteCount']				comment_user_nickname=comment_result['comments'][j]['nickname']				comment ={					'comment_user_id':comment_user_id,					'comment_user_nickname':comment_user_nickname,					'comment_score':comment_score,					'comment_like_num':comment_like_num,					'comment_content':comment_content,					'comment_time':comment_time,				}				# write_comment_to_mongo(url,comment)				commentlist.append(comment)			except:				continue		time.sleep(np.random.rand()/10)		return commentlist	except Exception:		# return UnknownSite(url)		print("商品评论获取失败！",url)if __name__ == '__main__':	start_time = time.time()	total_list_urls = []	TOTAL_COMMENT_NUM = 0	TOTAL_PRODUCT_NUM = get_url_num_from_mongo()	# TOTAL_PRODUCT_NUM =None	if TOTAL_PRODUCT_NUM is None:		product_list_urls = []		list_urls = get_list_url()		for j in range(len(list_urls)):  # #获取京东全部商品id			list_url = list_urls[j]			if "&" in list_url:				list_url = list_url.split('&')[0]			print("第",j,"个商品，还剩",len(list_urls) - j,"个，开始爬取此类商品：",list_url)			try:				total_page_num = get_total_page_num(list_url) # 某类商品全部页数,有可能一个商品也没有，所以直接跳过去				for i in range(1,int(total_page_num)+1):#循环总页数					product_list_url_temp = get_total_product_id(list_url,i)#获取第i页全部商品的id					# print(product_list_url_temp)					print("第",i,"页共有商品",len(product_list_url_temp),"个！当前时间为",time.strftime("%Y/%m/%d/%H:%M:%S"))					distinct_url(product_list_url_temp,total_list_urls)					time.sleep(np.random.rand()/10)				write_product_url_to_mongo(total_list_urls)				product_list_urls.extend(total_list_urls)				total_list_urls.clear()			except:				continue			print("截至目前为止共获取到商品：",len(product_list_urls),"个！")	ips = get_ip()	ip1 = ips[0].split(',')[0]	# vaild_times_1 = ips[0].split(',')[1]	ip2 = ips[1].split(',')[0]	# print(vaild_times_1)	for i in range(TOTAL_PRODUCT_NUM): # #获取京东全部商品信息和评论		product_url = get_one_product_url_from_mongo()		text = parse_page(product_url)		if text != None:			product = extract(product_url,text)			if product == None:				continue			# time.sleep(np.random.rand())			print("第",i,"个商品，还剩",TOTAL_PRODUCT_NUM-i,"个商品,商品链接为",product_url)		else:			continue		# ip_start_time = time.time()		# print(ips)		# vaild_times_2 = ips[1].split(',')[1]		while(True):			if ip_test(ip1):				# print('vaild_times_1',vaild_times_1,'******',int((time.time()-ip_start_time)*1000+3000))				# if(int(vaild_times_1)>int((time.time()-ip_start_time)*1000+3000)):				proxies = {"http":  "http://" + ip1,					"https": "https://" + ip1					}				print("IP1可用")				break			elif ip_test(ip2):				proxies = {"http":  "http://" + ip2,						"https": "https://" + ip2						}				print("IP2可用")				break			else:				print("重新申请IP组")				ips = get_ip()				# ip_start_time = time.time()				ip1 = ips[0].split(',')[0]				ip2 = ips[1].split(',')[0]				# vaild_times_1 = ips[0].split(',')[1]				# print(ip1,'***',vaild_times_1)		try:			print(proxies)			productid = product_url.split('/')[-1].split('.')[0]			comment_urls = ['https://sclub.jd.com/comment/productPageComments.action?productId='+productid+'&score=0&sortType=5&page={0}&pageSize=10&isShadowSku=0&rid=0&fold=1'.format(i) for i in range(100)]			'''京东默认最多显示100页的评论，0-99页，每页10条评论，也就是100*10，最多1000条评论'''			comments = []			for comment_url in comment_urls:				content = parse_comment_page(comment_url,proxies)				if content==None:					break				else:					comment = reviewExtract(comment_url,content)					comments.extend(comment)					TOTAL_COMMENT_NUM += len(comment)					# time.sleep(np.random.rand()/10)			result = {'productid':productid,'product':product,'comments':comments}			write_product_comments_to_mongo(result)			#print(result)			print("截至目前为止获取评论",TOTAL_COMMENT_NUM,"条！")			print("当前时间为",time.strftime("%Y/%m/%d/%H:%M:%S"))		except:			pass	print("一共获得京东网站评论",TOTAL_COMMENT_NUM,"条！")	end_time = time.time()	seconds =end_time - start_time	m, s = divmod(seconds, 60)	h, m = divmod(m, 60)  # 转化整个爬取时间为时分秒的形式	print("所花费总时间为%d时%02d分%02d秒" % (h, m, s))